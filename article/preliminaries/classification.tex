\subsection{Supervised Classification with Big Data}
% The problem of supervised classification bla bla bla...
% when dealing with a large amount of labelled data ect.

\subsubsection{Supervised learning}
Supervised learning is the process of learning the parameters of a linear model using a corpus of labelled data.
Given data in the form of a set of features and the value of their corresponding 'correct' class variable.  
Supervised learning uses machine learning algorithms to determine the features that together predict the value of the class variable.  

\subsubsection{Linear Models}
% regresssion vs. classification 

Linear models assign weights to the input features and computes the value of the class variable as a linear combination of the weights and the input features.
When the value of the class variable is continuous it is called a regression model and when it is discrete it is called a classification model.

% insert image showing a line and some data points.

\paragraph{Overfitting and Underfitting}
% overfitting vs. underfitting
Learning the parameters of a linear model can sometimes result a model that is too simple or too complex.
In the case of a model that is too simple, it is unable to catch the nuances presented in the data.
% insert image with a model that is too simple.  
When a model is too complex or the data used in training has a bias compare to the real world examples.
This can result in overfitting.
Overfitting means that the model is too closely fitted to the training data.
It will therefore provide good results on the training data but do increasingly poor on real world data.
% insert image of a model that overfits the data. 

\paragraph{Dealing with Overfitting}
% dividing data into test, training and validation sets.
% cross validation

One way of avoiding overfitting is by using a technique called cross validation.
With cross validation the data is divided into two seperate sets.
Where one is used for training and the other to test for overfitting. 
This way it becomes possible to detect when the model starts to overfit the training data.
A typical way to do this is by using a 60-30 split thereby using 60\% for training and 30\% for testing.
If the dataset is small, 30\% may be too much of a sacrifice.

% K-Fold cross validation
Another possibility is to use k-fold cross validation.
When applying this method the dataset is partitioned into a number of 'folds'.
The model is then trained for $k$ iterations.
In each iteration a different fold is used as the test set and the rest as the training data.
Thereby it becomes possible to use all of the data as both training and test set.
% Source: AI pool, macworth, p. 324
\begin{flushright}
\cite[p. 324]{AI2010}
\end{flushright}

\subsubsection{Logistic Regression}\label{sec:logistic}
% write: problem with logistic regression.
To predict the outcome of a match, we use a linear classification model.
The model consists of a hypothesis function that is linear in the parameters.

\[ \hat{y} = w_0 + \sum_{j=1}^{M-1} w_j \phi_j(x) \]

Where $w_j$ represents the impact of of $\phi_j(x)$ on the hypothesis,
$w_0$ is the intercept and $\phi_j$ is a basis function that
performs a transformation on the input features. 

% write: typical feature transformations -ref. bo slides.
% write: decision boundary.

In order to classify the result of the hypothesis function into wins and losses. 
We use the so called logistic or sigmoid function.

\[ \sigma(\textbf{x}) = \frac{1}{1+e^{- \textbf{x}}} \]

The sigmoid function squashes the result of the hypothesis function into an inverval between one and zero.
This is appropriate because we never want a prediction less than zero or greater than one on a binary feature.
Furthermore the sigmoid function has a simple derivative that makes it easy to work with when using iterative methods.

\[ \sigma'(x) = \sigma(x) \times (1-\sigma(x)) \] 

%\subsubsection{Cost Function}

%In order to find the weights that produce the most accurate classification.
%We minimise the squared error function.

% squared error with logistic regression becomes a non-convex cost function.
% write: alternative surrogate cost function.

%When using the hypothesis for logistic regression, we are no longer able to use the squared error function.

%\[ E_D = \sum_{n=1}^{N} \left(y_n - \textbf{w} \phi(\textbf{x}_n) \right)^2 \] 

%The problem with this % ???? 




\subsubsection{Regularisation}
% Write about L1 regularization, restructure subsubsection to be more general and present both types of regularization as equals.
To combat the problem of overfitting, we can use a method called L2-Regularisation.
In regularisation we penalise large weights, by adding the following term to the cost function.

\[ E_w = \lambda \sum_{j=1}^{M-1} w_j^2 \]

Where the regularisation constant $\lambda$ scales the penalty. 
The new cost function looks like this.

\[ E(\textbf{w})
  = E_D + E_w 
  = \sum_{n=1}^{N} \left(y_n - \textbf{w} \phi(\textbf{x}_n) \right)^2 + \lambda \sum_{j=1}^{M-1} w_j^2 \]

\begin{flushright}
\cite[online course]{courseraAI}
\end{flushright}

\subsubsection{Batch Gradient Descent}

Since we are using a large number of features, applying an analytic solution to find the weights quickly becomes impractical.
Therefore we use an iterative approach called batch gradient descent.

In gradient descent we iteratively update the weights by the gradient of the cost function, multiplied by a learning rate $\eta$  
\[ w_j \leftarrow w_j - \eta \nabla_w E(\textbf{w}) \]

\subsubsection{Stochastic Gradient Descent}\label{sec:stochastic}

When dealing with large data sets calculating the sum in the cost function becomes problematic.
 

% Non convergence of stochastic gradient descent


\begin{flushright}
\cite{Bishop2006}[p. ??]
\end{flushright}

% ###################################################################
%
%%\subsection{Logistic Regression}\label{sec:logistic}
%\subsection{Logistic Regression (old)}
%% logistic function
%
%For classification we use an activation function, whose purpose is to squash the result into the interval $(0,1)$.
%\[ f^{\overline{w}}(X_1, X_2, \dots X_n) = f(w_0 + w_1 \times X_1 + w_2 \times X_2 + \cdots + w_n \times X_n) \]
%
%\paragraph{Activation Functions}
%
%The most basic activation function is the so called step function defined as 
%\[ f(x) = \begin{cases}
%%	1 &\text{ if } f^{\overline{w}}(X_1,X_2, \text{ ... }, X_n) > 0 \\
%%	0 &\text{ if } f^{\overline{w}}(X_1,X_2, \text{ ... }, X_n) \leq 0 
%
%	1 &\text{ if } x \geq 0 \\
%	0 &\text{ if } x < 0 
%\end{cases}\]
%
%However the step function is not differentiable, and since this is a requirement for gradient descent we use another
%function called the sigmoid or logistic function.
%\[ f(x) = \frac{1}{1+e^{-x}} \]
%Unlike the previous activation function this function has a very simple derivative.
%
%\[ f'(x) = f(x) \times (1-f(x)) \]
%
%Which makes it easy to implement in a gradient descent algorithm.
%
%The cost function then becomes.
%\[ Error_E(\overline{w}) = \sum_{e \in E} \left(val(e,Y)-f\left(pval^{\overline{w}}(e,Y)\right)\right)^2 \]
%and the partial derivative becomes.
%\[ \frac{\partial \text{Error}_E(\overline{w})}{\partial w_i} 
%	= -2 \times \delta \times f'\left(\sum_i w_i \times val(e,X_i)\right) \times val(e,X_i) \]
%When using the sigmoid activation function the update step in gradient descent looks like this.
%\[ w_i := w_i + \eta \times \delta \times pval^{\overline{w}}(e,Y) \times \left(1 - pval^{\overline{w}}(e,Y)\right) \times val(e,X_i) \]
%
%Where $pval^{\overline{w}}(e,Y) = f(\sum_i w_i \times val(e,X_i))$.  
%
%
%\begin{flushright}
%\cite[p. 306-307]{AI2010}
%\end{flushright}
%
%
%\subsubsection{Regularisation}\label{sec:regular}
%To combat the problem of overfitting, we can use a method called regularisation.
%In regularisation we penalise large weights, by adding the following term to the cost function.
%
%% insert two pictures that show how regularization solves the overfitting problem.
%
%\[ \lambda \sum_{w_i \in \overline{w}} w_i^2 \]
%
%Where the regularisation constant $\lambda$ scales the penalty. 
%The new cost function looks like this.
%
%\[ Error_E(\overline{w}) = \sum_{e \in E} \left(val(e,Y) - pval^{\overline{w}}(e,Y)\right)^2 + \lambda \sum_{w_i \in \overline{w}} w_i^2 \]
%
%
%\begin{flushright}
%\cite[online course]{courseraAI}
%\end{flushright}

%\subsection{Large Datasets}
% Large datasets

% Stochastic regression












%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
