\section{Preliminaries}\label{sec:prelim}
In this section, theory and knowledge that is imperative for the understanding of this paper will be presented. In \Cref{sec:supervisedclassification}, several supervised learning concepts will be explained as it is the primary method used in this paper. In \Cref{sec:distributed}, the distributed file system Hadoop and the large scale data processing engine Apache Spark is introduced, as well as the motivation for using those tools in this project. Finally in \Cref{sec:sparklogreg}, Apache's logistic regression implementation is explored and explained. 

\subsection{Supervised Classification with Big Data}\label{sec:supervisedclassification}
In this section several supervised classification methods will be explored, including information on how they handle big data. 
% The problem of supervised classification bla bla bla...
% when dealing with a large amount of labelled data ect.

% write motivation for using supervised learning (DATA!!!) 
%In this section, theory or knowledge that is imperative for the understanding of this paper will be presented. In \Cref{sec:supervised learning}, several supervised learning concepts will be explained as it is the primary method used in this paper. In \Cref{sec:distributed}, \emph{Hadoop distributed filesystem} (HDFS) is briefly outlined followed by an introduction to Apache spark which is the cluster computing system mainly used throughout this paper.

\subsubsection{Supervised learning}
Supervised learning is the process of learning the parameters of a prediction model using a corpus of labelled data.
In supervised learning the data consists of objects with labels of the form 
$(\mathbf{x}_1,y_1),(\mathbf{x}_2,y_2), \dots, (\mathbf{x}_N,y_N)$.
Where $\mathbf{x}$ is a vector of inputs and $y$ is the class label for the data point.
The goal of supervised learning is to produce a model that is able to predict a label from the inputs alone.
When the domain of the label is continuous this is called a regression model and if it is discrete it is called a classification model.

\paragraph{Feature Representation}\label{sec:phi}
It is possible to design a lot of features for the model by combining and transforming the original inputs.
\begin{align*}
x & \rightarrow \phi(x) \\
  & \rightarrow (\phi_1(x), \phi_2(x), \phi_3(x), \phi_4(x)) \\
  & \rightarrow (1, x_1, x_1^2, x_1x_2)
\end{align*} 
Above we have a few examples of how an input vector $x$ can be transformed.
The examples include a constant, the identity, a polynomial and the combination of two inputs.

\subsubsection{Linear Models}
% insert image showing a line and some data points.

When attempting to predict a continuous value, one standard method is by using linear regression.
The linear regression model generates a prediction $\hat{y}$ of the value $y$. This prediction is found with the hypothesis function $h(x)$ by assigning weights $\textbf{w} = w_1, w_2, \dots w_M$ to the features and computing their sum.
\[h_{\textbf{w}}(x) = \hat{y} = w_0 + \sum_{i=1}^M w_i \phi_i(x) \]
Where $w_0$ is a special case called the intercept, however to avoid having this special case we add the feature $\phi_0(x)=1$, which allows us to use the intercept as a normal feature. 
The next question becomes, how do we assign a value to the weights?
Given the amount of data we have available, it would be preferable to learn these weights from the data. 

\paragraph{Linear Regression}
Since the goal is to fit the hypothesis function to a training set, linear regression uses the following loss function given the data $D$ and the weights $\textbf{w}$.
\[ L_D(\textbf{w}) = \sum_{n=1}^N (y_n-\hat{y}_n)^2 = \sum_{n=1}^N \left(y_n - \sum_{i=0}^M w_i \phi_i(x_n)\right)^{2} \] 
The best parameters are those that minimise the loss function. Since the function is convex, the best parameters can be found either analytically by setting the derivative to 0, or by using an iterative method introduced later called gradient descent.

When giving a prediction $\hat{y}$ on the value of a discrete binary label we use a thresholding function:
\[
\hat{y} = 
\begin{cases}
  1 &\text{ if } h_w(x) > \tau \\
  0 &\text{ if } h_w(x) \leq \tau  
\end{cases}
\] 
Although this may work in some cases, a dataset with a few outliers can significantly skew the decision boundary.
In order to combat this problem we introduce a new linear model for classification called logistic regression.

\paragraph{Logistic Regression}\label{sec:logistic}

In logistic regression we use the so called logistic or sigmoid function to squash the prediction into the interval $[0,1]$.
\[ \hat{y} = \sigma(h_w(x)) = \frac{1}{1+e^{-h_w(x)}} \]
By doing this, a single outlying data point is no longer able to skew the decision boundary of the hypothesis function.
However this comes at a cost.
When combining the sigmoid and squared sums loss function, the resulting loss function is no longer convex, which means that we may only find a local minimum when searching for the best parameters.
Logistic regression solves this problem by introducing an error function.
\[E_w(y,h_w(x)) = \begin{cases}
	-ln(\sigma(h_w(x))) &\text{if y = 1}\\	
	-ln(1-\sigma(h_w(x))) &\text{if y = 0}
%	-ln(\sum_{w_i \in w} w_i \phi_i(x_n)) &\text{if y = 1}\\	
%	-ln(1-\sum_{w_i \in w} w_i \phi_i(x_n)) &\text{if y = 0}
\end{cases}\]
When the error of the hypothesis function is 0, the error function returns 0 as well. When the error of the hypothesis function goes towards 1, the penalty caused by the error function goes towards infinity.
When using the error function to define the loss function
$$L_D(w) = \sum_{n=1}^N E_w(y_n, h_w(x_n))$$
the loss function once again becomes convex.

\subsubsection{Overfitting and Underfitting}
% overfitting vs. underfitting
Learning the parameters of a linear model can result in a model that is either too simple or too complex.
If the model is too simple, it will not be able to catch the nuances present in the data, causing low predictive power. This behaviour is called underfitting.
% insert image with a model that is too simple.  
When a model is too complex, it will fit the training data too well, resulting in bad predictive power on previously unseen data. This behaviour is called overfitting. 

To evaluate how well a model fit some data, we can split the data into a training and test set. Evaluation can be done on both data sets and be compared. If the model is better at predicting the training data compared to the test data, some degree of overfitting is happening. If however the model has the same predictive power on both sets, we either have an underfitted or well fitted model. A common approach is to use more data for training than testing. For instance by using a 70/30 split.

Another possibility is to use k-fold cross validation.
When applying this method the dataset is partitioned into $k$ numbers of ``folds'', with one test fold and $k-1$ training folds.
The model is then trained for $k$ iterations, such that each iteration use a different test fold.
Thereby it becomes possible to use all of the data as both training and test set. In the following section we present a solution to overfitting linear models.
% Source: AI pool, macworth, p. 324

\subsubsection{Regularisation}

To combat the problem of overfitting, we can also use a method called regularisation.
In regularisation we penalise large weights, by adding a second term to the loss function.
Generally speaking there exist two types of regularisation L1 and L2.
L1-regularisation is good at encouraging sparse solutions when there are many irrelevant features whereas L2-regularisation is better when most features are relevant.

\begin{itemize}
\item L1-Regularisation: $L_w = \lambda \sum_{m} \vert w_m \vert $
\item L2-Regularisation: $L_w = \frac{\lambda}{2} \sum_{m} {w_m}^2$
\end{itemize}

Where the regularisation constant $\lambda$ scales the regularisation penalty in relation to the data loss function. For logistic regression with L2-Regularisation the combined loss function looks like this.
\[ L(\mathbf{w})
  = L_D(\mathbf{w}) + L_w 
  = \sum_{n=1}^N E_w(y_n, h_w(x_n)) + \frac{\lambda}{2} \sum_{m=1}^{M} {w_m}^2 \] 

\subsubsection{Batch Gradient Descent}\label{sec:batch}

As we wish to use a large number of features, applying an analytic solution to find the weights quickly becomes impractical.
Therefore we need to use an iterative approach.
One such method is called batch gradient descent.

In batch gradient descent we iteratively update the weights $w_j$ for $0 \leq j \leq M$, by a value proportional to the partial derivative of the loss function until the value converges to a local minimum of $L(\textbf{w})$, i.e.\ when $w_{j}^{(i)} = w_j^{(i+1)}$ for all weights.
%\[ w_j \leftarrow w_j - \eta \frac{\partial L(\textbf{w})}{ \partial w_j} 
%	= w_j - \eta \frac{\partial(\sum_{n=1}^N E_w(y_n, h_w(x_n)))}{ \partial w_j}  \]
\[w_j^{(i)} = w_j^{(i-1)} - \eta \frac{\partial L(\mathbf{w})}{\partial w_j^{(i-1)}}
       = w_{j}^{(i-1)} - \eta \frac{\partial \left(\displaystyle\sum_{n=1}^N E_{w^{(i-1)}}(y_n, h_{w^{(i-1)}}(x_n)) + \frac{\lambda}{2}\sum_{m=1}^M {w^{(i-1)}_m}^2 \right) }{\partial w_j^{(i-1)}} \]  
Because the loss function is convex, there is only one minimum i.e. the global minimum. 
The value $\eta$ represent the learning rate i.e.\ how big a step the algorithm should take in each iteration.
This value need to be chosen wisely as a value that is too small will increase the number of iterations required and a value that is too large could cause the algorithm to diverge.

When dealing with a large dataset this method runs into a problem, namely that computing the partial derivative requires summing over all of the data points in the training set in every iteration. 

\subsubsection{Stochastic Gradient Descent}\label{sec:stochastic}

Stochastic gradient descent solves the problem of large dataset by updating the weights on each data point.
This also makes the algorithm a good choice for updating a model online with continuously incoming data. 

In the algorithm for stochastic gradient descent we first randomly shuffle all objects in the dataset $D$.
We then iteratively update the weights in a way similar to batch gradient descent but using a different equation, here shown without regularisation.

\[ w_j^{(i)} = w_j^{(i-1)} - \eta \frac{\partial E_{w^{(i-1)}}(y_n, h_{w^{(i-1)}}(x_n))}{\partial w_j^{(i-1)}} \]

For each object $(x_n, y_n)$ in the dataset.

One problem with stochastic gradient descent is that the algorithm fail to converge in some cases, but instead moves the weight around in an area close to the minimum value.
Therefore it becomes necessary to use a separate strategy to accomplish this.
A well known method is to do what is called simulated annealing on the learning rate.
Simulated annealing works by reducing the value of the learning rate as the number of iterations increases.

$$\eta_i = \frac{1}{(1+i)^\alpha}$$

With an $\alpha$ chosen in the interval $(0.5,1]$. 


The pseudocode for the algorithm can be seen in \Cref{lst:sgt}.
\begin{listing}[H]
\begin{minted}{python}
def StochasticGradientDescent(W,E,D):
  Shuffle(D)

  Repeat Until hasConverged(W,oldW):
    for x in D do:
      for w in W do: 
        w := w - eta * partial(E(W,x),w)
\end{minted}
\caption{Stochastic Gradient Descent}
\label{lst:sgt}
\end{listing}

\paragraph{Mini-Batch Gradient Descent}\label{sec:mini-batch}
Because stochastic gradient descent updates the weights using individual data points, the gradient found could just as well move away from a minimum value, as it could move towards it. 
To alleviate this problematic behaviour, there exist a variant of stochastic gradient descent called mini-batch gradient descent. 
The only difference between them is that each update is an average of ten updates in mini-batch gradient descent.







%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
