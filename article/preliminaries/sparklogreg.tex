\subsection{Spark's implementation of logistic regression}\label{sec:sparklogreg}
The purpose of this section is to inspect Sparkâ€™s implementation of logistic regression.
The models available in Spark's MLlib are all implemented in scala, all the models are made available in Pyspark through the PythonMLLibAPI module~\cite{hadoopIntro}.
\Cref{lst:py_logreg} shows the implementation of the train function from the LogisticRegressionWithSGD data is the RDD, miniBatchFraction is the amount of data being used in each iteration 1.0 is 100\% of the data. The remaining inputs should be self explanatory.   
\begin{listing}[H]
\begin{minted}[linenos, firstnumber=219]{python}
class LogisticRegressionWithSGD(object):
@classmethod
def train(cls, data, iterations=100, step=1.0, miniBatchFraction=1.0,
initialWeights=None, regParam=0.01, regType="l2", intercept=False,
validateData=True):
\end{minted}
\label{lst:py_logreg}
\caption{Lines from classification.py}
\end{listing}

The main scripts involved with logistic regression in spark are.
\begin{itemize}
\item LogisticRegression.scala
\item GradientDescent.scala
\item Updater.scala
\item Gradient.scala
\end{itemize}

In \Cref{lst:gd_logreg} a LogisticGradient and SquaredL2Updater is created and passed to GradientDescent which returns the weights needed to create the model.
\begin{listing}[H]
\begin{minted}[linenos, firstnumber=211]{scala}
private val gradient = new LogisticGradient()
private val updater = new SquaredL2Updater()
override val optimizer = new GradientDescent(gradient, updater)
.setStepSize(stepSize)
.setNumIterations(numIterations)
.setRegParam(regParam)
.setMiniBatchFraction(miniBatchFraction)
\end{minted}
\caption{Lines from LogisticRegression.scala}
\label{lst:gd_logreg}
\end{listing}

The main function involved with calculating the weights is runMiniBatchSGD from line 149 in GradientDescent.scala. The runMiniBatchSGD is shown in \Cref{lst:runMiniBatchSGD}. 
\begin{listing}[H]
\begin{minted}[linenos, firstnumber=181]{scala}
var regVal = updater.compute(
weights, Vectors.dense(new Array[Double](weights.size)), 0, 1, regParam)._2
for (i <- 1 to numIterations) {
val bcWeights = data.context.broadcast(weights)
// Sample a subset (fraction miniBatchFraction) of the total data
// compute and sum up the subgradients on this subset (this is one map-reduce)
val (gradientSum, lossSum, miniBatchSize) = data.sample(false, miniBatchFraction,
42 + i).treeAggregate((BDV.zeros[Double](n), 0.0, 0L))(
seqOp = (c, v) => {
// c: (grad, loss, count), v: (label, features)
val l = gradient.compute(v._2, v._1, bcWeights.value, Vectors.fromBreeze(c._1))
(c._1, c._2 + l, c._3 + 1)
},
combOp = (c1, c2) => {
// c: (grad, loss, count)
(c1._1 += c2._1, c1._2 + c2._2, c1._3 + c2._3)
})

if (miniBatchSize > 0) {
/**
* NOTE(Xinghao): lossSum is computed using the weights from the previous iteration
* and regVal is the regularization value computed in the previous iteration as well.
*/
stochasticLossHistory.append(lossSum / miniBatchSize + regVal)
val update = updater.compute(
weights, Vectors.fromBreeze(gradientSum / miniBatchSize.toDouble), stepSize,
i, regParam)
weights = update._1
regVal = update._2
\end{minted}
\caption{Lines from GradientDescent.scala}
\label{lst:runMiniBatchSGD}
\end{listing}
The lines 187 to 209 in side the for-loop in \Cref{lst:runMiniBatchSGD} is a map reduce calculating loss accordingly to the weights, and then updates the weights to minimize loss with respect to the regularization parameter. Lines 191 and 192 use the LogisticGradient object created by the LogisticRegression object to compute the loss based on the label, features, current weights and gradient. On line 205 the updater computes new weights and a new regVal from the given weights, average gradient, stepSize, iteration and regularization factor.
From \Cref{lst:sparkloss} taken from compute function in Gradient.scala it is clear that loss computed is a logistic loss.  
\begin{listing}[H]
\begin{minted}[linenos, firstnumber=173]{scala}
val margin = -1.0 * dot(data, weights)
val multiplier = (1.0 / (1.0 + math.exp(margin))) - label
axpy(multiplier, data, cumGradient)
if (label > 0) {
// The following is equivalent to log(1 + exp(margin)) but more numerically stable.
MLUtils.log1pExp(margin)
} else {
MLUtils.log1pExp(margin) - margin
}
\end{minted}
\caption{Lines from Gradient.scala}
\label{lst:sparkloss}
\end{listing}
The default updater used for logistic regression is a SquaredL2Updater from Updater.scala
the code ran in the compute function is the following:

\begin{listing}[H]
\begin{minted}[linenos, firstnumber=138]{scala}
override def compute(
weightsOld: Vector,
gradient: Vector,
stepSize: Double,
iter: Int,
regParam: Double): (Vector, Double) = {
// add up both updates from the gradient of the loss (= step) as well as
// the gradient of the regularizer (= regParam * weightsOld)
// w' = w - thisIterStepSize * (gradient + regParam * w)
// w' = (1 - thisIterStepSize * regParam) * w - thisIterStepSize * gradient
val thisIterStepSize = stepSize / math.sqrt(iter)
val brzWeights: BV[Double] = weightsOld.toBreeze.toDenseVector
brzWeights :*= (1.0 - thisIterStepSize * regParam)
brzAxpy(-thisIterStepSize, gradient.toBreeze, brzWeights)
val norm = brzNorm(brzWeights, 2.0)

(Vectors.fromBreeze(brzWeights), 0.5 * regParam * norm * norm)
}
\end{minted}
\caption{Lines from Updater.scala}
\label{lst:updatercal}
\end{listing}

