\subsection{Spark's implementation of logistic regression}\label{sec:sparklogreg}
The purpose of this section is to inspect Sparkâ€™s implementation of logistic regression.
In Spark, all machine learning models are implemented in scala \cite{sparkml}.\Cref{lst:py_logreg} shows the implementation of the train function from the LogisticRegressionWithSGD. \texttt{data} is the RDD, \texttt{miniBatchFraction} is the amount of data being used in each iteration 1.0 is 100\% of the data. The remaining inputs should be self explanatory.   
\begin{listing}[H]
\begin{minted}[linenos, firstnumber=219]{python}
class LogisticRegressionWithSGD(object):
  @classmethod
  def train(cls, data, iterations=100, step=1.0, miniBatchFraction=1.0,
  initialWeights=None, regParam=0.01, regType="l2", intercept=False,
    validateData=True):
\end{minted}
\caption{Relevant code from classification.py}
\label{lst:py_logreg}
\end{listing}
The main scripts involved with logistic regression in spark are as follows.
\begin{itemize}
\item LogisticRegression.scala
\item GradientDescent.scala
\item Updater.scala
\item Gradient.scala
\end{itemize}

The main function involved with calculating the weights is \texttt{runMiniBatchSGD} from line 149 in GradientDescent.scala. Part of the \texttt{runMiniBatchSGD} is shown in \Cref{lst:runMiniBatchSGD}. 
\begin{listing}[H]
\begin{minted}[linenos, firstnumber=181]{scala}
var regVal = updater.compute(
    weights, Vectors.dense(new Array[Double](weights.size)), 0, 1, regParam)._2
  for (i <- 1 to numIterations) {
    val bcWeights = data.context.broadcast(weights)
    // Sample a subset (fraction miniBatchFraction) of the total data
    // compute and sum up the subgradients on this subset (this is one map-reduce)
    val (gradientSum, lossSum, miniBatchSize) = data.sample(false, miniBatchFraction,
        42 + i).treeAggregate((BDV.zeros[Double](n), 0.0, 0L))(
    seqOp = (c, v) => {
      // c: (grad, loss, count), v: (label, features)
      val l = gradient.compute(v._2, v._1, bcWeights.value, Vectors.fromBreeze(c._1))
      (c._1, c._2 + l, c._3 + 1)
    },
    combOp = (c1, c2) => {
      // c: (grad, loss, count)
        (c1._1 += c2._1, c1._2 + c2._2, c1._3 + c2._3)
    })
    if (miniBatchSize > 0) {
      stochasticLossHistory.append(lossSum / miniBatchSize + regVal)
      val update = updater.compute(
        weights, Vectors.fromBreeze(gradientSum / miniBatchSize.toDouble), stepSize,
        i, regParam)
      weights = update._1
      regVal = update._2
    }
    ...
  }
  (weights, stochasticLossHistory.toArray)
\end{minted}
\caption{Relevant code from GradientDescent.scala}
\label{lst:runMiniBatchSGD}
\end{listing}
The lines 187 to 197 inside the for-loop in \Cref{lst:runMiniBatchSGD} is a MapReduce, which calculates loss according to the weights, features and label. Line 200 updates the weights to minimise loss with respect to the regularisation parameter.
 
\begin{listing}[H]
\begin{minted}[linenos, firstnumber=173]{scala}
val margin = -1.0 * dot(data, weights)
val multiplier = (1.0 / (1.0 + math.exp(margin))) - label
axpy(multiplier, data, cumGradient)
if (label > 0) {
  // The following is equivalent to log(1 + exp(margin)) but more numerically stable.
  MLUtils.log1pExp(margin)
} else {
  MLUtils.log1pExp(margin) - margin
}
\end{minted}
\caption{Relevant code from Gradient.scala}
\label{lst:sparkloss}
\end{listing}
\Cref{lst:sparkloss} shows the compute function from Gradient.scala. It is clear that the loss calculated is a logistic loss.
 


\begin{listing}[H]
\begin{minted}[linenos, firstnumber=138]{scala}
override def compute(
  weightsOld: Vector, gradient: Vector, stepSize: Double, iter: Int, regParam: Double):
     (Vector, Double) = {
  // add up both updates from the gradient of the loss (= step) 
  val thisIterStepSize = stepSize / math.sqrt(iter)
  val brzWeights: BV[Double] = weightsOld.toBreeze.toDenseVector
  brzWeights :*= (1.0 - thisIterStepSize * regParam)
  brzAxpy(-thisIterStepSize, gradient.toBreeze, brzWeights)
  val norm = brzNorm(brzWeights, 2.0)
  (Vectors.fromBreeze(brzWeights), 0.5 * regParam * norm * norm)
}
\end{minted}
\caption{Relvant code from Updater.scala}
\label{lst:updatercal}
\end{listing}

The default \texttt{updater} used for logistic regression is a L2-regularisation updater from Updater.scala
the code for this compute function is shown in \Cref{lst:updatercal}



