\section{Setting up the Cluster}
\label{sec:hadoop}
To set up a cluster with equal settings to ours, all the machines must be running Debian Wheezy 7.0 32-bit (linux) and connected via switch with static ip. In addiction, one machine must be chosen as the master and the following packages must be installed on all machines:
\begin{itemize}
\item openssh 
\item openssh-server 
\item openjdk-7-jre 
\item openjdk-7-jdk 
\end{itemize}
Now create a new user for the cluster setup, this is considered good practice. The \emph{username} must be the same name across all machines.
\lstset{language=bash}

\begin{minted}{bash}
adduser ``username''
\end{minted}
To easier work with the IP's of the machines, set up a file called \emph{hosts}:
\begin{verbatim}
127.0.0.1 localhost

ip node1
ip node2
ip node3
...
\end{verbatim}
Which is saved in the hosts directory: 
\begin{verbatim}
/etc/hosts
\end{verbatim}
Following that, each machine needs ssh configured, where \emph{node-id} is the alias created in the \emph{hosts} file. Run the following commands in the terminal of each master to each slave and from each slave to master:
\begin{minted}{bash}
  ssh-keygen
  ssh-copy-id ``username@node-id''
\end{minted}

\subsection{Setting up Hadoop}
All of the following steps must be done on all machines. To set up Hadoop, create a hadoop group, then add the users previously created to that group:
\begin{minted}{bash}
  addgroup hadoop
  adduser ``username'' hadoop
\end{minted}
At this point Hadoop 2.6.0 32-bit should be downloaded and installed in the path \textsf{/usr/local/hadoop}. We then change the owner of the hadoop directory so we have the correct access:
\begin{minted}{bash}
  chown -R username:username /usr/local/hadoop
\end{minted}
The following will be put in \emph{/home/username/.bashrc} file:
\begin{verbatim}
export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-i386
export HADOOP_INSTALL=/usr/local/hadoop
export PATH=$PATH:$HADOOP_INSTALL/bin
export PATH=$PATH:$HADOOP_INSTALL/sbin
export HADOOP_MAPRED_HOME=$HADOOP_INSTALL
export HADOOP_COMMON_HOME=$HADOOP_INSTALL
export HADOOP_HDFS_HOME=$HADOOP_INSTALL
export YARN_HOME=$HADOOP_INSTALL
\end{verbatim}
In the file \emph{hadoop-env.sh} found in \textsf{/usr/local/hadoop/etc/hadoop/} the line \emph{JAVA\_HOME} should be replaced with \emph{export JAVA\_HOME=/usr/lib/java-7-openjdk-i386}.
In addition the \emph{core-site.xml} which is found in \textsf{/usr/local/hadoop/etc/hadoop/} needs an added property inside the configuration tag: 
\begin{verbatim}
<property>
  <name>fs.defaultFS</name>
  <value>hdfs://MASTER:9000</value>
</property>
\end{verbatim}
\emph{MASTER} needs to be replaced with the hostname for the master node. Another file that needs added properties is \emph{hdfs-site.xml}, which can be found in the folder \textsf{/usr/local/hadoop/etc/hadoop/}:
\begin{verbatim}
<property>
  <name>dfs.namenode.data.dir</name>
  <value>file:/home/username/data/hdfs/namenode</value>
</property>

<property>
  <name>dfs.datanode.data.dir</name>
  <value>file:/home/username/data/hdfs/datanode</value>
</property>
\end{verbatim}
These should also be added in the configuration tag.
The final thing that needs for Hadoop to run is for the master to know the slaves. Create a file called \emph{slaves} in \textsf{/usr/local/hadoop/etc/hadoop/} with all the hostnames of slaves listed separated by newline:
\begin{verbatim}
node1
node2
...
\end{verbatim}

\subsection{Setting up Apache Spark}
All of the following steps must done on all machines. To get Spark 1.2.1 running, download the hadoop 32-bit precompiled version and install it to \textsf{/usr/local/spark} and again run the command to set up the correct rights:
\begin{lstlisting}
  chown -R username:username/usr/local/spark
\end{lstlisting}
The final step for setting up Apache Spark is copying the previous \emph{slaves} file into the folder \textsf{/usr/local/spark/conf/}.

\subsection{Starting the cluster}
To start the cluster the master has to run the following commands:
\begin{minted}{bash}
  start-dfs.sh
  cd /usr/local/spark
  ./sbin/start-all.sh
\end{minted}
Now the cluster is ready to use.

\subsection{Adding files}
To add files to the file system use the following script:
\begin{minted}{bash}
  string = $(hdfs dfs -ls hdfs://node1:9000/)
  for file in "file path"*
  do
  name = $(file##*/)

  if [[$string == *$name* ]]
  then
    echo 'file exists'
  else
    hdfs dfs -copyFromLocal $file hdfs://node1:9000/$name
    echo 'adding new file'
  fi 

  done
  \end{minted}
This will add all the files, that does not already exist, from the given folder to the hdfs file system.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
