\subsection{Motivation}\label{sec:motivation}
As seen in \Cref{sec:mlandonlinevideogames} some work has been done using machine learning for video games, however LoL is a rather unexplored game. 
In this paper we will attempt to develop a method for LoL, by using machine learning and big data techniques, which will help players and game developers analyse the game, and make better choices. In \Cref{sec:intro} the central parts of LoL is described, where we show that LoL is a complex game, as the number of choices each player can make is very large. Due to the high complexity, the use of big data techniques and a large quantity of match data, has the potential to increase the predictive power of machine learning techniques. Large amounts of data is provided by Riot through a public developer API. The API grants access to information about millions of LoL matches, which is not possible to manage effectively through traditional data processing methods.

\subsubsection{Big Data Problem}\label{sec:big_data_problem}
A big data problem is characterised by the three factors: \emph{volume}, \emph{velocity}, and \emph{variety}. These factors are ever changing as a result of technology improvements. For instance a data volume in the gigabyte range were once impossible for a single machine to manage, whereas modern hardware is more than capable. As such, big data is a somewhat vague term used to describe a system where one of these factors challenge traditional data processing methods. As we have extracted around 456GB of match data, we have a great volume of data. The variety of the data is on the other hand low as it is on the same format and it is all from the same source. Velocity is not an issue either, as the data we work with is a static dataset. Working with LoL match data could however be a velocity problem, as millions of players continuously produce new match data. Given the data used for this paper, Riot generated 40GB of new match data per day in Western Europe alone. Because patches that alter the game are occasionally applied to the game; patches can change how champions work or even add new ones, the classifier will need to be updated, making velocity an issue to consider. We are however not going to consider this issue and merely focus on predicting outcomes of matches. Therefore our only big data concern is related to volume~\cite{madden2012databases}.

A high volume of data can cause a number of problems when using a single computer. The data can simply be too large to store on a single machine unless a lot of storage units have been attached. Using many storage units on a single computer is however not very fault tolerant as if one unit dies, that data is lost. Another issue is the processing power, additional computational power could introduce bottleneck issues. These issues can be solved by using a group of machines with a distributed file system. By having the data distributed, several copies can be maintained so the data is not lost if a single machine or storage unit crashes. In addition, using multiple machines adds computational power without adding a data transfer bottleneck issues, since each machine can do computation on its local data.
%This also means that certain machine learning algorithms are more applicable, since the problem will have to be split up and computed in parallel. 

\subsection{Overview}\label{sec:overview}
In \Cref{sec:prelim} some preliminary knowledge and information to the understanding of the work done in this paper will be presented. This includes several machine learning methods and an explanation of MapReduce, Hadoop Distributed Filesystem and Apache Spark. In addition the inner workings of Apache's implementation of logistic regression are explored. 
In \Cref{sec:features} the used data will be presented followed by the selection and transformation of features. It also includes information about the sparsity of the features, and some suggestions to representation are presented as well.  
In \Cref{sec:cluster}, information about a cluster of machines will be explored, this includes the setup, resource usage and performance. 
The results of all the experiments will be presented in \Cref{sec:testing}, where both initial experiments performed on a single computer and experiments on the cluster will be present. After the results have been presented, a discussion of knowledge extraction will be given to show what we can conclude on the results. 
Finally in \Cref{sec:conclusion} the conclusion of the project will be presented along with suggestions for future work that will expand the project.



%A high volume of data poses a number of problem on a single computer. Let us consider these problems: One problem may be that the data is too big to be stored. This can be solved by buying additional storage units. Secondly, we are limited in processing power, adding more computational power is not economical and may introduce bottleneck issues. A way to solve these issues is to split and distribute the data across multiple machines, which means having several units for processing with their own storage. By assigning every machine to process the data stored at its own storage the computation time can be decreased.
%A single node is chosen as the \textit{master}, which schedules tasks at the other nodes called \textit{slaves}. A such setup is called a \textit{cluster}, and is greatly scaleable for a wide range of applications, namely those applications that can be parallelized.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
