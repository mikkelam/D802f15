\subsection{Motivation}\label{sec:motivation}
As seen in \Cref{sec:mlandonlinevideogames}, a lot of work has been done using machine learning for video games, however LoL is a rather unexplored game. 
In this paper, we will attempt to develop a method for LoL, which will help players pick champions that give their team the greatest probability of winning. In \Cref{sec:onlinevideogames}, LoL is described and it is clear that the number of decisions each player can make is large. Due to this large number of decisions each player can make, a large quantity of match data is required to get a proper representation. 

The data is provided by RG who, through their public API, grants access to an enormous amount of data, including match data.
The large quantity of data means that we will have a \emph{big data problem}, which means the traditional data processing methods can not effectively be applied. Storage of big data and finding a machine learning method that scales with large data are some of the things that will be explored in this paper.

\subsubsection{Big Data Problem}\label{sec:big_data_problem}
A big data problem is characterised by the three factors: \emph{volume}, \emph{velocity}, and \emph{variety}. As we have gained access to more than 400GB of match data, we have a great volume of data. The variety of the data is on the other hand low as it is on the same format and it is all from the same source. Velocity is not an issue either, as the data we are going to work with is a static dataset. It could however be expanded to a velocity problem as well, because patches that alter the game are occasionally applied to the game. Patches can change how champions work or even add new ones. The model could then continuously be updated for every new patch, making velocity an issue to consider. Riot Games generates 40GB of new match data per day in Western Europe alone. We are not going to consider this issue and merely focus on finding out, to which extend the outcome of a match can be predicted on a single patch. Therefore, our only big data concern is related to volume~\cite{madden2012databases}.

A high volume of data can cause a number of problems when using a single computer. The data can simply be too large to store on a single machine unless a lot of storage units have been attached. Using many storage units on a single computer is however not very fault tolerant as if one unit dies, that data is lost. Another issue is the processing power, additional computational power could introduce bottleneck issues. These issues can be solved by using a group of machines with a distributed file system, where the data can be distributed across. By having the data distributed, several copies could be maintained so the data is not lost if a machine or storage unit crashes. In addition several machines adds the computational power without adding the same bottleneck issues since each machine will only do the computation on the local data.
This also means that certain machine learning algorithms are more applicable, since the problem will have to be split up and computed in parallel. 

\subsection{Method}
In \Cref{sec:prelim} some knowledge and information imperative to the understanding of the work done in this paper will be presented. This includes several machine learning methods and an explanation of Hadoop Distributed Filesystem and Apache Spark. In \Cref{sec:features} the data will be presented followed by the selection and transformation of features. In \Cref{sec:cluster}, information about a cluster of machines will be explored, this includes the setup, resource usage and performance. The results of all the experiments will be presented in \Cref{sec:test}, where both experiments performed on a single computer and on the cluster will be present. After the results have been presented, a discussion of knowledge extraction will be given to show what we can conclude on the results. Finally in \Cref{sec:conclusion} the conclusion of the project will be presented along with suggestions for future work that will expand the project.



%A high volume of data poses a number of problem on a single computer. Let us consider these problems: One problem may be that the data is too big to be stored. This can be solved by buying additional storage units. Secondly, we are limited in processing power, adding more computational power is not economical and may introduce bottleneck issues. A way to solve these issues is to split and distribute the data across multiple machines, which means having several units for processing with their own storage. By assigning every machine to process the data stored at its own storage the computation time can be decreased.
%A single node is chosen as the \textit{master}, which schedules tasks at the other nodes called \textit{slaves}. A such setup is called a \textit{cluster}, and is greatly scaleable for a wide range of applications, namely those applications that can be parallelized.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
