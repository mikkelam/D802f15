\section{Cluster}\label{sec:cluster}
In this section, the cluster that will be doing the computations will be outlined. In \Cref{sec:clustersetup} the setup of the cluster and the available resources are listed, followed by a profile of the resource usage in \Cref{sec:profile}. In \Cref{sec:benchmark} the performance of the cluster will be tested against a standalone computer, both doing word count as a measurement. In \Cref{sec:speedup} the speedup from using additional nodes for computations will be measured.

\subsection{Cluster Setup}\label{sec:clustersetup}

The cluster used for this work consist of four nodes, one master and three worker nodes. The master is master both in terms of cluster management on Spark and storage manegement on HDFS. Because of the limited resources for the project and small size of the cluster and dataset, some of the common fault tolerance functionality of Hadoop and Spark has not been employed. Most significantly, only one replication of data exist across HDFS, which naturally put large parts of the data at risk. There are two unique setups the cluster consist of which only differs on the CPU:\@
\begin{description}
\item[CPU:] Dual core intel e8400 3Ghz or Quad core q9400 2.66Ghz
\item[Storage:] 220 GB
\item[Memory:] 4 GB DDR2 RAM
\end{description}

Where the master and one worker node has a dual core and the two remaining worker nodes have 4 cores. The machines are linked together using a switch capable of 200Mbit full duplex, which sets a potential one-way bandwidth of 12.5MB/s. Naturally, when large amounts of data is being managed, it is necessary to manage the jobs, so that worker nodes strictly or dominantly work on local data only.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
