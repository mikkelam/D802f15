\subsection{Complications of distributed computing}\label{sec:complications}
Using distributed computing comes with some limitations which can cause some issues. Here we will outline the problems that we experienced when attempting to do computations on the cluster.

\subsubsection{Immutability of data}
Data on the HDFS is immutable, which means once the data has been moved to the HDFS, it can not be altered in any way, the only action available is appending additional data. We experienced this when we learned that there were duplicates of matches in the data, which were described in \Cref{sec:features}. In addition we needed the data to have the same patch version to take into account that champions had no change to their skills. Since our first batch of data suffered from both of these issues we unsuccessfully tried to remove the JSONs that caused these conflicts. This resulted in a new batch of data being gathered where matches that could cause conflict were sorted away before being transferred to the HDFS.   

%Within HDFS, all data is immutable. Although appending to a file is slightly supported, there is no way to delete or modify the content of a file.
%When parsing the matches as JSON objects, we want to discard some of the matches, either because they contain indvalid JSON, or because they have a different patch version or match type than what we are looking for. Since data from multiple matches are stored in the same file, we are not able remove those matches within HDFS.

\subsubsection{Precomputing feature mapping}
The labeled data that Spark uses for training must be a labeled vector of features, where each feature is represented by an index.
It is important, that all nodes represent the same feature with the same index. Therefore, before any features are extracted from a match, all nodes create identical maps, that map all possible features to a unique index. The disadvantage is that the size of the mapping may be larger than the size of the features we actually meet.
Another solution would be to use a \emph{shared} map function, where entries are created on the fly as features are found in the data.
However, the \emph{broadcast} variables in Hadoop are read-only, and can thus not be updated on the fly.
Therefore, a shared map function has to be precomputed, but having a shared map function increases the need for the much slower communication between nodes.


\subsubsection{Similarity with parallel programming}
\subsubsection{Shared memory vs. network message passing}
\subsubsection{Transforming/referencing RDD can only happen one at a time}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
