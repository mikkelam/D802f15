\subsection{Logistic Regression}\label{sec:logistic}
% logistic function

For classification we use an activation function, whose purpose is to squash the result into the interval $(0,1)$.
\[ f^{\overline{w}}(X_1, X_2, \dots X_n) = f(w_0 + w_1 \times X_1 + w_2 \times X_2 + \cdots + w_n \times X_n) \]

\paragraph{Activation Functions}

The most basic activation function is the so called step function defined as 
\[ f(x) = \begin{cases}
%	1 &\text{ if } f^{\overline{w}}(X_1,X_2, \text{ ... }, X_n) > 0 \\
%	0 &\text{ if } f^{\overline{w}}(X_1,X_2, \text{ ... }, X_n) \leq 0 

	1 &\text{ if } x \geq 0 \\
	0 &\text{ if } x < 0 
\end{cases}\]

However the step function is not differentiable, and since this is a requirement for gradient descent we use another
function called the sigmoid or logistic function.
\[ f(x) = \frac{1}{1+e^{-x}} \]
Unlike the previous activation function this function has a very simple derivative.

\[ f'(x) = f(x) \times (1-f(x)) \]

Which makes it easy to implement in a gradient descent algorithm.

The cost function then becomes.
\[ Error_E(\overline{w}) = \sum_{e \in E} \left(val(e,Y)-f\left(pval^{\overline{w}}(e,Y)\right)\right)^2 \]
and the partial derivative becomes.
\[ \frac{\partial \text{Error}_E(\overline{w})}{\partial w_i} 
	= -2 \times \delta \times f'\left(\sum_i w_i \times val(e,X_i)\right) \times val(e,X_i) \]
When using the sigmoid activation function the update step in gradient descent looks like this.
\[ w_i := w_i + \eta \times \delta \times pval^{\overline{w}}(e,Y) \times \left(1 - pval^{\overline{w}}(e,Y)\right) \times val(e,X_i) \]

Where $pval^{\overline{w}}(e,Y) = f(\sum_i w_i \times val(e,X_i))$.  


\begin{flushright}
\cite[p. 306-307]{AI2010}
\end{flushright}


\subsubsection{Regularisation}\label{sec:regular}
To combat the problem of overfitting, we can use a method called regularisation.
In regularisation we penalise large weights, by adding the following term to the cost function.

% insert two pictures that show how regularization solves the overfitting problem.

\[ \lambda \sum_{w_i \in \overline{w}} w_i^2 \]

Where the regularisation constant $\lambda$ scales the penalty. 
The new cost function looks like this.

\[ Error_E(\overline{w}) = \sum_{e \in E} \left(val(e,Y) - pval^{\overline{w}}(e,Y)\right)^2 + \lambda \sum_{w_i \in \overline{w}} w_i^2 \]


\begin{flushright}
\cite[online course]{courseraAI}
\end{flushright}

%\subsection{Large Datasets}
% Large datasets

% Stochastic regression












%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
