\subsection{Supervised Classification with Big Data}
% The problem of supervised classification bla bla bla...
% when dealing with a large amount of labelled data ect.

\subsubsection{Supervised learning}
Supervised learning is the process of learning the parameters of a linear model using a corpus of labelled data.
Given data

% overfitting vs. underfitting
% regresssion vs. classification 


\subsection{Logistic Regression}\label{sec:logistic}

% write: problem with logistic regression.
To predict the outcome of a match, we use a linear classification model.
The model consists of a hypothesis function that is linear in the parameters.

\[ \hat{y} = w_0 + \sum_{j=1}^{M-1} w_j \phi_j(x) \]

Where $w_j$ represents the impact of of $\phi_j(x)$ on the hypothesis,
$w_0$ is the intercept and $\phi_j$ is a basis function that
performs a transformation on the input features. 

% write: typical feature transformations -ref. bo slides.

In order to classify the result of the hypothesis function into wins and losses. 
We use the so called logistic or sigmoid function.

\[ \sigma(\textbf{x}) = \frac{1}{1+e^{- \textbf{x}}} \]

The sigmoid function squashes the result of the hypothesis function into an inverval between one and zero.
This is appropriate because we never want a prediction less than zero or greater than one on a binary feature.
Furthermore the sigmoid function has a simple derivative that makes it easy to work with when using iterative methods.

\[ \sigma'(x) = \sigma(x) \times (1-\sigma(x)) \] 

\subsubsection{Cost Function}

In order to find the weights that produce the most accurate classification.
We minimise the squared error function.

\[ E_D = \sum_{n=1}^{N} \left(y_n - \textbf{w} \phi(\textbf{x}_n) \right)^2 \] 

% squared error with logistic regression becomes a non-convex cost function.
% write: alternative surrogate cost function.


\subsubsection{Regularisation}
To combat the problem of overfitting, we can use a method called L2-Regularisation.
In regularisation we penalise large weights, by adding the following term to the cost function.

\[ E_w = \lambda \sum_{j=1}^{M-1} w_j^2 \]

Where the regularisation constant $\lambda$ scales the penalty. 
The new cost function looks like this.

\[ E(\textbf{w})
  = E_D + E_w 
  = \sum_{n=1}^{N} \left(y_n - \textbf{w} \phi(\textbf{x}_n) \right)^2 + \lambda \sum_{j=1}^{M-1} w_j^2 \]

\begin{flushright}
\cite[online course]{courseraAI}
\end{flushright}

\subsubsection{Batch Gradient Descent}

Since we are using a large number of features, applying an analytic solution to find the weights quickly becomes impractical.
Therefore we use an iterative approach called batch gradient descent.

In gradient descent we iteratively update the weights by the gradient of the cost function, multiplied by a learning rate $\eta$  
\[ w_j \leftarrow w_j - \eta \nabla_w E(\textbf{w}) \]

\subsubsection{Stochastic Gradient Descent}\label{sec:stochastic}

When dealing with large data sets calculating the sum in the cost function becomes problematic.
 

% Non convergence of stochastic gradient descent


\begin{flushright}
\cite{Bishop2006}[p. ??]
\end{flushright}

% ###################################################################
%
%%\subsection{Logistic Regression}\label{sec:logistic}
%\subsection{Logistic Regression (old)}
%% logistic function
%
%For classification we use an activation function, whose purpose is to squash the result into the interval $(0,1)$.
%\[ f^{\overline{w}}(X_1, X_2, \dots X_n) = f(w_0 + w_1 \times X_1 + w_2 \times X_2 + \cdots + w_n \times X_n) \]
%
%\paragraph{Activation Functions}
%
%The most basic activation function is the so called step function defined as 
%\[ f(x) = \begin{cases}
%%	1 &\text{ if } f^{\overline{w}}(X_1,X_2, \text{ ... }, X_n) > 0 \\
%%	0 &\text{ if } f^{\overline{w}}(X_1,X_2, \text{ ... }, X_n) \leq 0 
%
%	1 &\text{ if } x \geq 0 \\
%	0 &\text{ if } x < 0 
%\end{cases}\]
%
%However the step function is not differentiable, and since this is a requirement for gradient descent we use another
%function called the sigmoid or logistic function.
%\[ f(x) = \frac{1}{1+e^{-x}} \]
%Unlike the previous activation function this function has a very simple derivative.
%
%\[ f'(x) = f(x) \times (1-f(x)) \]
%
%Which makes it easy to implement in a gradient descent algorithm.
%
%The cost function then becomes.
%\[ Error_E(\overline{w}) = \sum_{e \in E} \left(val(e,Y)-f\left(pval^{\overline{w}}(e,Y)\right)\right)^2 \]
%and the partial derivative becomes.
%\[ \frac{\partial \text{Error}_E(\overline{w})}{\partial w_i} 
%	= -2 \times \delta \times f'\left(\sum_i w_i \times val(e,X_i)\right) \times val(e,X_i) \]
%When using the sigmoid activation function the update step in gradient descent looks like this.
%\[ w_i := w_i + \eta \times \delta \times pval^{\overline{w}}(e,Y) \times \left(1 - pval^{\overline{w}}(e,Y)\right) \times val(e,X_i) \]
%
%Where $pval^{\overline{w}}(e,Y) = f(\sum_i w_i \times val(e,X_i))$.  
%
%
%\begin{flushright}
%\cite[p. 306-307]{AI2010}
%\end{flushright}
%
%
%\subsubsection{Regularisation}\label{sec:regular}
%To combat the problem of overfitting, we can use a method called regularisation.
%In regularisation we penalise large weights, by adding the following term to the cost function.
%
%% insert two pictures that show how regularization solves the overfitting problem.
%
%\[ \lambda \sum_{w_i \in \overline{w}} w_i^2 \]
%
%Where the regularisation constant $\lambda$ scales the penalty. 
%The new cost function looks like this.
%
%\[ Error_E(\overline{w}) = \sum_{e \in E} \left(val(e,Y) - pval^{\overline{w}}(e,Y)\right)^2 + \lambda \sum_{w_i \in \overline{w}} w_i^2 \]
%
%
%\begin{flushright}
%\cite[online course]{courseraAI}
%\end{flushright}

%\subsection{Large Datasets}
% Large datasets

% Stochastic regression












%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
