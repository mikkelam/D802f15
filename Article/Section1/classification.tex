\section{Classification}\label{sec:classification}

% intro

% features - target(class)

\subsection{Linear Regression}
% linear regression
The goal of linear regression is to fit a linear function to a set of input-output pairs. 
If the input features are $X_1,X_2 \text{ ... } X_n$ and weights $\overline{w} = w_0, w_1 \text{ ... } w_n$.

$$f^{\overline{w}}(X_1,X_2, ... X_n) = w_0 + w_1 X_1 + w_2 X_2 + ... w_n X_n $$ 

We will learn the function for each target feature $Y$ seperately, so given a set $E$ of examples.
The function $val(e,X_i)$ give us the value of the feature $X_i$ in example $e \in E$. 

The function 
\begin{align}
pval^{\overline{w}}(e,Y) &= w_0 + w_1 \text{ val}(e,X_1) + w_2 \text{ val}(e,X_2) + \text{ ... } w_n \text{ val}(e,X_n) \\
&= \sum^n_{i=0} w_i \text{ val}(e,X_i)
\end{align}

give us the predicted value for the target feature $Y$ for the training example $e$.
In order to avoid having a special case we assume a feature $X_0$ with $val(e,X_0)=1$.
\begin{flushright}
\cite[p. 304]{AI2010}
\end{flushright}


\subsection{Cost Function}
% cost function
A cost function is a measure on how much it will "cost" us if the prediction for a given example is wrong.

Three basic cost functions exist: the absolute error function, the sum-of-squares error function and 
the worst-case error function.
 
The absolute error function provides the sum of all absolute errors on the examples in $E$.

$$AbsErr = \sum_{e \in E}\sum_{Y \in T} |val(e,Y) - pval(e,Y)|$$

This function is allways non-negative and only zero when $Y$ is predicted correctly in all examples. 

The sum-of-squares error function assigns a larger error the further a prediction is from the actual value.

$$SSErr = \sum_{e \in E}\sum_{Y \in T} (val(e,Y) - pval(e,Y))^2$$

The worst-case error function gives the maximum error predicted for all examples.

$$MaxErr = \max_{e \in E}\max_{Y \in T} | val(e,Y) - pval(e,Y) |$$

\begin{flushright}
\cite[p. 290-291]{AI2010}
\end{flushright}

Using the sum-of-squares error function we get the cost function.

\begin{align}
Error_E(\overline{w}) &= \sum_{e \in E} (val(e,Y)-pval^{\overline{w}}(e,Y))^2 \\
&= \sum_{e \in E} (val(e,Y)-\sum_{i=0}^n w_i \times val(e,X_i))^2
\end{align}

The goal when learning a classification model, is to chose the weights that minimize the error,
that is.

$$\text{argmin}_{\overline{w}} \text{ Error}_E(\overline{w})$$

In the linear case these weights can be found analytically by taking the partial derivative of the cost function with respect 
to each weight, setting it equal to zero and solving for the weight.
However when using a more complex model or a large set of features, a more generel iterative solution is required.

\subsubsection{Gradient descent}

Another way of finding the weights that minimize the cost function, is called gradient descent.
Gradient descent is an iterative approach that in each step takes the decreases the weights in
proportion to their partial derivative.

$$w_i = w_i - \eta \times \frac{\partial \text{ Error}_E(\overline{w})}{\partial w_i}$$ 

where $\eta$ is the learining rate.

For the sum-of-squares error function we get

\begin{align}
w_i &:= w_i - \eta \times \frac{\partial \sum_{e \in E}(val(e,Y) - pval^{\overline{w}}(e,Y))^2}{\partial w_i} \\
&= w_i - \eta \times -2 \times (val(e,Y) - pval^{\overline{w}}(e,Y)) \times val(e,X_i) \\
&= w_i + \eta \times \delta \times val(e,X_i)
\end{align}

Where $\delta = val(e,Y) - pval^{\overline{w}}(e,Y)$ and the constant 2 has been absorbed into $\eta$.

\begin{flushright}
\cite[p. 305]{AI2010}
\end{flushright}

\subsection{Logistic Regression}
% logistic function

For classification we use an activation function, whose purpose is to squash the result into the interval $(0,1)$.

$$f^{\overline{w}}(X_1, X_2, \text{ ... } X_n) = f(w_0 + w_1 \times X_1 + w_2 \times X_2 + \text{ ... } + w_n \times X_n)$$

\subsubsection{Activation Functions}

The most basic activation function is the so called step function defined as 

$$f(x) = \begin{cases}
%	1 &\text{ if } f^{\overline{w}}(X_1,X_2, \text{ ... }, X_n) > 0 \\
%	0 &\text{ if } f^{\overline{w}}(X_1,X_2, \text{ ... }, X_n) \leq 0 

	1 &\text{ if } x \geq 0 \\
	0 &\text{ if } x < 0 
\end{cases}$$

However the step function is not differentiable, and since this is a requirement for gradient descent we use another
function called the sigmoid or logistic function.

$$f(x) = \frac{1}{1+e^{-x}}$$

Unlike the previous activation function this function has a very simple derivative.

$$f'(x) = f(x) \times (1-f(x))$$

Which makes it easy to implement in a gradient descent algorithm.

The cost function then becomes.

$$Error_E(\overline{w}) = \sum_{e \in E} (val(e,Y)-f(pval^{\overline{w}}(e,Y)))^2$$ 

and the partial derivative becomes.

$$\frac{\partial \text{Error}_E(\overline{w})}{\partial w_i} 
	= -2 \times \delta \times f'(\sum_i w_i \times val(e,X_i)) \times val(e,X_i)$$

When using the sigmoid activation function the update step in gradient descent looks like this.

$$w_i := w_i + \eta \times \delta \times pval^{\overline{w}}(e,Y) \times (1 - pval^{\overline{w}}(e,Y)) \times val(e,X_i)$$

Where $pval^{\overline{w}}(e,Y) = f(\sum_i w_i \times val(e,X_i))$.  


\begin{flushright}
\cite[p. 306-307]{AI2010}
\end{flushright}


\subsection{Regularization}
% regularization


\subsection{Large Datasets}
% Large datasets












